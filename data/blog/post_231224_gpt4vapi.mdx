---
title: GPT4V test
date: '2023-12-24'
tags: ['all-doc-ai','chatGPT','vastai', 'stable Diffusion']
draft: false
summary: 'GPT_tools'
authors: ['gypsyR']
---


## GPT4V 日本語ドキュメント
以下は、GPT-4のイメージ理解機能について学ぶ方法をステップバイステップで説明した日本語のマークダウン形式のREADMEドキュメントです。  

こちらにサンプルプログラムを置いてます。
[Githubページ](https://github.com/ryo-ponsan/gpt4v-image-processing)👈

---

# GPT-4で画像を理解する方法を学ぶ

## 概要

GPT-4にビジョン機能（画像理解機能）を組み合わせることで、モデルは画像を入力として受け取り、それについての質問に答えることができます。従来の言語モデルシステムはテキストという単一の入力モダリティに限定されていましたが、この機能によりGPT-4を使用できる範囲が広がります。

現在、GPT-4ビジョンは「gpt-4-vision-preview」モデルを通じて、GPT-4にアクセスできるすべての開発者に利用可能です。また、画像入力をサポートするために更新されたChat Completions APIを通じて利用できます。ただし、Assistants APIは現在画像入力をサポートしていません。

以下の点に注意してください：

- GPT-4 Turbo with visionは、会話に自動的に挿入されるシステムメッセージのため、GPT-4 Turboとは少し異なる振る舞いをすることがあります。
- GPT-4 Turbo with visionはGPT-4 Turboプレビューモデルと同一で、テキストタスクでは同様に高い性能を発揮しますが、ビジョン機能が追加されています。
- ビジョンはモデルが持つ多くの能力のうちの一つです。
- 現在、GPT-4 Turbo with visionは`message.name`パラメーター、関数/ツール、`response_format`パラメーターをサポートしておらず、デフォルトで低い`max_tokens`が設定されていますが、これはオーバーライドすることができます。

## クイックスタート

画像は主に2つの方法でモデルに提供されます：画像へのリンクを渡すか、リクエスト内で直接base64エンコードされた画像を渡すかです。ユーザー、システム、アシスタントのメッセージ内で画像を渡すことができます。現在、最初のシステムメッセージでの画像サポートはありませんが、将来的には変更される可能性があります。

### 例：画像内の内容は何ですか？

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
  model="gpt-4-vision-preview",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "この画像には何がありますか？"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0])
```

モデルは画像内に存在するものについての一般的な質問に答えることが得意です。画像内のオブジェクト間の関係を理解していますが、画像内の特定のオブジェクトの位置に関する詳細な質問に答えるようにはまだ最適

化されていません。例えば、車の色が何であるかや、冷蔵庫の中身を基に夕食のアイデアを尋ねることはできますが、部屋の画像を見せて椅子がどこにあるかを尋ねると、正確に答えない場合があります。

モデルの限界を理解し、視覚理解を応用できるユースケースを探求する際には注意が必要です。

続きの内容を日本語のマークダウン形式のREADMEドキュメントにします。

---

## Base64エンコードされた画像のアップロード

ローカルにある画像や画像セットをモデルに渡す場合、Base64エンコード形式で渡すことができます。以下はその実行例です。

```python
import base64
import requests

# OpenAI API キー
api_key = "YOUR_OPENAI_API_KEY"

# 画像をエンコードする関数
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# 画像へのパス
image_path = "path_to_your_image.jpg"

# Base64文字列の取得
base64_image = encode_image(image_path)

headers = {
  "Content-Type": "application/json",
  "Authorization": f"Bearer {api_key}"
}

payload = {
  "model": "gpt-4-vision-preview",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "この画像には何がありますか？"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}"
          }
        }
      ]
    }
  ],
  "max_tokens": 300
}

response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)

print(response.json())
```

## 複数の画像入力

Chat Completions APIは、Base64エンコード形式または画像URLとして、複数の画像入力を取り込み、処理することができます。モデルは各画像を処理し、それらすべての情報を使用して質問に回答します。

### 例：複数の画像入力

```python
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
  model="gpt-4-vision-preview",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "これらの画像には何がありますか？ それらの間に違いはありますか？",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
          },
        },
      ],
    }
  ],
  max_tokens=300,
)
print(response.choices[0])
```

ここでは、モデルに同じ画像の2つのコピーが示され、それぞれの画像または両方の画像について質問に答えることができます。

こちらが、GPT-4の画像理解機能の詳細に関する日本語のマークダウン形式のREADMEドキュメントの続きです。

---

## 低解像度または高解像度の画像理解

`detail`パラメーターを制御することで、モデルが画像を処理し、テキストによる理解を生成する方法をコントロールできます。`detail`には`low`、`high`、`auto`の3つのオプションがあります。デフォルトでは、モデルは`auto`設定を使用し、画像入力のサイズを見て`low`または`high`設定を使用するかを決定します。

- `low`は「高解像度モード」を無効にします。モデルは画像の低解像度512px x 512pxバージョンを受け取り、65トークンの予算で画像を表現します。これにより、APIはより高速な応答を返し、高い詳細が必要でないユースケースのために入力トークンを少なく消費します。
- `high`は「高解像度モード」を有効にします。これにより、モデルはまず低解像度の画像を見てから、入力画像のサイズに基づいて512px四方の詳細なクロップを作成します。詳細なクロップはそれぞれ65トークンの2倍の予算（合計129トークン）を使用します。

### 解像度レベルの選択

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
  model="gpt-4-vision-preview",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "この画像には何がありますか？"},
        {
          "type": "image_url",
          "image_url": {
            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
            "detail": "high"
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0].message.content)
```

## 画像の管理

Chat Completions APIはAssistants APIと異なり、ステートフル（状態を保持する）ではありません。つまり、モデルに渡すメッセージ（画像を含む）を自分自身で管理する必要があります。同じ画像をモデルに複数回渡す場合、APIへのリクエストを行うたびに画像を渡す必要があります。

長期間にわたる会話の場合、base64よりもURL経由で画像を渡すことをお勧めします。モデルのレイテンシーは、事前に画像をダウンサイズして、期待される最大サイズよりも小さくすることで改善されます。低解像度モードでは、512px x 512pxの画像が期待されます。高解像度モードでは、画像の短い側が768px以下、長い側が2,000px以下であることが期待されます。

モデルによって画像が処理された後、それはOpenAIのサーバーから削除され、保持されません。OpenAI APIを通じてアップロードされたデータは、私たちのモデルをトレーニングするために使用されません。


以下に、GPT-4のビジョン機能に関する制限事項を日本語のマークダウン形式のREADMEドキュメントとして記載します。

---

## 制限事項

GPT-4のビジョン機能は強力で多くの状況で使用できますが、モデルの制限を理解することが重要です。以下は、私たちが認識しているいくつかの制限事項です：

- **医療画像**：モデルはCTスキャンなどの専門的な医療画像の解釈には適しておらず、医療アドバイスには使用しないでください。
- **非英語**：モデルは日本語や韓国語などの非ラテン文字のテキストを含む画像の処理において最適に機能しない場合があります。
- **小さなテキスト**：画像内のテキストの可読性を向上させるために拡大しますが、重要な詳細をトリミングしないでください。
- **回転**：モデルは回転したり上下逆のテキストや画像を誤解釈することがあります。
- **視覚要素**：モデルはグラフや色やスタイル（実線、破線、点線など）が変化するテキストを理解するのに苦労する場合があります。
- **空間的推論**：モデルはチェスの位置など、正確な空間的位置付けを必要とするタスクで苦戦します。
- **精度**：モデルは特定のシナリオで誤った説明やキャプションを生成する可能性があります。
- **画像の形状**：モデルはパノラマや魚眼画像で苦戦します。
- **メタデータとリサイズ**：モデルは元のファイル名やメタデータを処理せず、分析前に画像をリサイズします。これにより、元の寸法が影響を受けます。
- **カウント**：画像内のオブジェクトの数をおおよそカウントする場合があります。
- **CAPTCHA**：安全上の理由から、CAPTCHAの送信をブロックするシステムを実装しています。

## 画像入力のコスト計算

画像入力はテキスト入力と同様にトークンで計量され、請求されます。与えられた画像のトークンコストは、サイズと各`image_url`ブロックの`detail`オプションによって決まります。`detail: low`のすべての画像は、それぞれ85トークンのコストがかかります。`detail: high`の画像はまず2048 x 2048の正方形に収まるようにスケールされ、アスペクト比を維持します。次に、画像の最短側が768pxになるようにスケールされます。最後に、画像がいくつの512px正方形で構成されているかを数えます。これらの各正方形は170トークンのコストがかかります。最終的な合計には常に85トークンが加算されます。

以下は上記を示す例です。

- `detail: high`モードの1024 x 1024の正方形画像は、765トークン

のコストがかかります。
  - 1024は2048未満なので、最初のリサイズはありません。
  - 最短側が1024なので、画像を768 x 768に縮小します。
  - 画像を表現するために4つの512px正方形タイルが必要なので、最終的なトークンコストは170 * 4 + 85 = 765です。
- `detail: high`モードの2048 x 4096画像は、1105トークンのコストがかかります。
  - 画像を2048の正方形に収まるように1024 x 2048に縮小します。
  - 最短側が1024なので、さらに768 x 1536に縮小します。
  - 6つの512pxタイルが必要なので、最終的なトークンコストは170 * 6 + 85 = 1105です。
- `detail: low`モードの4096 x 8192画像は、85トークンのコストがかかります。
  - 入力サイズに関わらず、低解像度画像は固定コストです。


以下に、GPT-4のビジョン機能に関するよくある質問（FAQ）を日本語のマークダウン形式のREADMEドキュメントとして記載します。

---

## よくある質問（FAQ）

### Q: GPT-4で画像機能をファインチューニングできますか？
A: いいえ、現時点ではGPT-4の画像機能をファインチューニングすることはサポートしていません。

### Q: GPT-4を使用して画像を生成できますか？
A: いいえ、画像を生成するにはDALL-E 3を使用し、GPT-4ビジョンプレビューを画像理解に使用できます。

### Q: どのタイプのファイルをアップロードできますか？
A: 現在、PNG (.png)、JPEG (.jpeg および .jpg)、WEBP (.webp)、およびアニメーションされていないGIF (.gif)をサポートしています。

### Q: アップロードできる画像のサイズに制限はありますか？
A: はい、画像アップロードは画像あたり20MBに制限されています。

### Q: アップロードした画像を削除できますか？
A: いいえ、モデルによって処理された後、自動的に画像を削除します。

### Q: GPT-4ビジョンに関する考慮事項について詳しく知りたいのですが、どこで学べますか？
A: GPT-4ビジョンシステムカードに評価、準備、緩和作業の詳細が記載されています。

### Q: GPT-4ビジョンでCAPTCHAをブロックするシステムはありますか？
A: はい、CAPTCHAの提出をブロックするシステムをさらに実装しています。

### Q: GPT-4ビジョンのレート制限はどのように機能しますか？
A: 画像はトークンレベルで処理されるため、処理する画像ごとにトークンごとの分限（TPM）制限にカウントされます。画像ごとのトークン数を決定するために使用される式の詳細については、「コストの計算」セクションを参照してください。

### Q: GPT-4ビジョンは画像メタデータを理解できますか？
A: いいえ、モデルは画像メタデータを受け取りません。

### Q: 画像が不明瞭な場合、どうなりますか？
A: 画像が曖昧または不明瞭な場合、モデルはそれを解釈するために最善を尽くします。ただし、結果の精度は低下する可能性があります。良い経験則として、平均的な人が低解像度/高解像度モードで使用される解像度で画像内の情報を見ることができない場合、モデルも同様にできません。