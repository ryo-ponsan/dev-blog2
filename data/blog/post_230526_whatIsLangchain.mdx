---
title: LangChainとは + クイックスタート
date: '2023-05-26'
tags: ['LangChain', 'chatGPT', 'OpenAI', 'API']
draft: false
summary: 'LangChain'
authors: ['gypsyR']
---

- [コンセプトについて](#1)
- [クイックスタート](#2)

<a id="1"></a>

# [Concepts](https://python.langchain.com/en/latest/getting_started/concepts.html)

LangChain では、以下のような一連の概念や用語が一般的に使用されます。これらの概念は、LLM（Language Model）アプリケーションの開発において重要な役割を果たします。

## 1. Chain of Thought（思考の連鎖）

Chain of Thought（CoT）は、モデルが一連の中間的な推論ステップを生成するように促すプロンプト技術です。この振る舞いを誘発する非公式な方法としては、プロンプトに「ステップバイステップで考えてみましょう」と含めることがあります。

- [Chain-of-Thought Paper](https://arxiv.org/abs/2106.02504)
- [Step-by-Step Paper](https://arxiv.org/abs/2106.02504)

## 2. Action Plan Generation（行動計画生成）

Action Plan Generation は、言語モデルを使用して行動を生成するプロンプト技術です。これらの行動の結果は、その後の行動を生成するために言語モデルにフィードバックされます。

- [WebGPT Paper](https://arxiv.org/abs/2106.02504)
- [SayCan Paper](https://arxiv.org/abs/2106.02504)

## 3. ReAct

ReAct は、Chain-of-Thought プロンプトと行動計画生成を組み合わせたプロンプト技術です。これにより、モデルはどの行動を取るべきかを考え、その行動を取るようになります。

- [ReAct Paper](https://arxiv.org/abs/2106.02504)
- [LangChain Example](https://python.langchain.com/en/latest/modules/chains/generic/router.html)

## 4. Self-ask（自己問い合わせ）

Self-ask は、Chain-of-Thought プロンプトを基にしたプロンプト方法です。この方法では、モデルは自身にフォローアップの質問を明示的に行い、その回答は外部の検索エンジンによって提供されます。

- [Self-ask Paper](https://arxiv.org/abs/2106.02504)
- [LangChain Example](https://python.langchain.com/en/latest/modules/chains/generic/router.html)

## 5. Prompt Chaining（プロンプトチェーン）

Prompt Chaining は、複数の LLM 呼び出しを組み合わせ、一つのステップの出力が次のステップの入力となるようにするものです。

- [PromptChainer Paper](https://arxiv.org/abs/2106.02504)
- [Language Model Cascades](https://arxiv.org/abs/2106.02504)
- [ICE Primer Book](https://arxiv.org/abs/2106.02504)
-

（続き）

[Socratic Models](https://arxiv.org/abs/2106.02504)

## 6. Memetic Proxy（模倣的プロキシ）

Memetic Proxy は、モデルが特定の方法で応答するように促すために、議論をモデルが知っているコンテキストでフレーム化することを指します。例えば、学生と教師の間の会話としてフレーム化します。

- [Memetic Proxy Paper](https://arxiv.org/abs/2106.02504)

## 7. Self Consistency（自己一貫性）

Self Consistency は、多様な推論パスをサンプリングし、最も一貫した回答を選択するデコーディング戦略です。Chain-of-thought プロンプトと組み合わせると最も効果的です。

- [Self Consistency Paper](https://arxiv.org/abs/2106.02504)

## 8. Inception（インセプション）

Inception は、First Person Instruction（第一人称指示）とも呼ばれます。これは、プロンプトの中にモデルの応答の開始部分を含めることで、モデルが特定の方法で考えるように促すものです。

- [Inception Example](https://arxiv.org/abs/2106.02504)

## 9. MemPrompt（メムプロンプト）

MemPrompt は、エラーやユーザーフィードバックのメモリを保持し、それらを使用してミスの繰り返しを防ぎます。

- [MemPrompt Paper](https://arxiv.org/abs/2106.02504)

以上が、LangChain で一般的に使用される概念と用語の説明です。これらの概念は、LLM アプリケーションの開発において重要な役割を果たします。

<a id="2"></a>

# クイックスタート

以下は、指定されたリンクの「Quickstart Guide」の日本語訳です。

---

# LangChain のクイックスタートガイド

## インストール

LangChain を始めるためには、以下のコマンドで LangChain をインストールします。

```bash
pip install langchain
# または
conda install langchain -c conda-forge
```

## 環境設定

LangChain を使用するには、通常、一つ以上のモデルプロバイダ、データストア、API などとの統合が必要です。この例では、OpenAI の API を使用するため、まずその SDK をインストールする必要があります。

次に、環境変数をターミナルで設定します。

```bash
export OPENAI_API_KEY="..."
```

または、Jupyter ノートブック（または Python スクリプト）内から以下のように設定することもできます。

```python
import os
os.environ["OPENAI_API_KEY"] = "..."
```

API キーを動的に設定したい場合は、OpenAI クラスを初期化する際に`openai_api_key`パラメータを使用できます。例えば、各ユーザーの API キーなどです。

```python
from langchain.llms import OpenAI
llm = OpenAI(openai_api_key="OPENAI_API_KEY")
```

## 言語モデルアプリケーションの構築: LLMs

LangChain をインストールし、環境を設定したら、言語モデルアプリケーションの構築を開始できます。LangChain は、言語モデルアプリケーションを構築するために使用できる多くのモジュールを提供しています。モジュールは、より複雑なアプリケーションを作成するために組み合わせることができます。また、シンプルなアプリケーションの場合は、個々に使用することもできます。

### LLMs: 言語モデルからの予測の取得

LangChain の最も基本的な構成要素は、ある入力に対して LLM を呼び出すことです。これを行うための簡単な例を見てみましょう。この目的のために、会社が何を作っているかに基づいて会社の名前を生成するサービスを構築していると仮定しましょう。これを行うためには、まず LLM ラッパーをインポートする必要があります。

```python
from langchain.llms import OpenAI
```

次に、任意の引数でラッパーを初期化します。この例では、出力をよりランダ

ムにしたいので、高い温度で初期化します。

```python
llm = OpenAI(temperature=0.9)
```

これで、何かの入力に対して呼び出すことができます！

```python
text = "What would be a good company name for a company that makes colorful socks?"
print(llm(text))
```

LangChain 内で LLMs を使用する方法の詳細については、LLM のゲッティングスタートガイドを参照してください。

### プロンプトテンプレート: LLMs のためのプロンプトの管理

LLM を呼び出すことは素晴らしい第一歩ですが、それは始まりに過ぎません。通常、LLM をアプリケーションで使用するとき、ユーザーの入力を直接 LLM に送信するのではなく、おそらくユーザーの入力を取得し、プロンプトを構築し、それを LLM に送信しています。例えば、前の例では、私たちが渡したテキストは、カラフルな靴下を作る会社の名前を尋ねるためにハードコーディングされていました。この仮想的なサービスでは、私たちは会社が何をするかを説明するユーザーの入力だけを取得し、その情報でプロンプトをフォーマットしたいと考えています。これは LangChain を使って簡単に行うことができます！まず、プロンプトテンプレートを定義しましょう。

```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["product"],
    template="What is a good name for a company that makes {product}?",
)
```

これがどのように動作するかを見てみましょう！ `.format`メソッドを呼び出してフォーマットすることができます。

```python
print(prompt.format(product="colorful socks"))
```

これは "What is a good name for a company that makes colorful socks?" を出力します。

詳細については、プロンプトのゲッティングスタートガイドをご覧ください。

### チェーン: LLMs とプロンプトを多段階のワークフローで組み合わせる

これまで、PromptTemplate と LLM のプリミティブをそれぞれ単独で扱ってきました。しかし、もちろん、実際のアプリケーションは単一のプリミティブではなく、それらの組み合わせです。LangChain のチェーンは、リンクで構成されており、これらのリンクは LLMs

のようなプリミティブまたは他のチェーンのいずれかです。

最も基本的なタイプのチェーンは LLMChain で、これは PromptTemplate と LLM から構成されます。前述の例を拡張して、LLMChain を構築することができます。これは、ユーザーの入力を取得し、それを PromptTemplate でフォーマットし、フォーマットされたレスポンスを LLM に渡します。

```python
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

llm = OpenAI(temperature=0.9)
prompt = PromptTemplate(
    input_variables=["product"],
    template="What is a good name for a company that makes {product}?",
)

from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)
```

これで、製品を指定するだけでそのチェーンを実行できます！

```python
chain.run("colorful socks")
# -> '\n\nSocktastic!'
```

これで最初のチェーン、つまり LLM チェーンが完成しました。これはチェーンの中でも比較的シンプルなタイプの一つですが、より複雑なチェーンを扱うための基礎を理解するのに役立ちます。詳細については、チェーンのゲッティングスタートガイドをご覧ください。

# エージェント: ユーザー入力に基づいてチェーンを動的に呼び出す

これまで見てきたチェーンは、事前に決定された順序で実行されます。エージェントはもはやそうではありません: それらは LLM を使用してどのアクションを取るべきか、そして何の順序で取るべきかを決定します。アクションは、ツールを使用してその出力を観察するか、ユーザーに戻るかのいずれかです。

正しく使用すると、エージェントは非常に強力になることができます。このチュートリアルでは、最もシンプルで最高レベルの API を通じてエージェントを簡単に使用する方法を示します。

エージェントをロードするためには、以下の概念を理解する必要があります：

- ツール: 特定の任務を実行する関数。これは、Google 検索、データベース検索、Python REPL、他のチェーンなどのようなものです。ツールのインターフェースは現在、入力として文字列を持つことが期待されている関数で、出力も文字列です。
- LLM: エージェントを動かす言語モデル。
- エージェント: 使用するエージェント。これは、サポートされているエージェントクラスを参照する文字列であるべきです。このノートブックは最もシンプルで最高レベルの API に焦点を当てているため、これは標準のサポートされているエージェントの使用のみをカバーしています。カスタムエージェントを実装したい場合は、カスタムエージェントのドキュメンテーションを参照してください（近日公開予定）。

この例では、SerpAPI Python パッケージもインストールする必要があります。

```bash
pip install google-search-results
```

そして、適切な環境変数を設定します。

```python
import os
os.environ["SERPAPI_API_KEY"] = "..."
```

それでは始めましょう！

```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.llms import OpenAI

# 最初に、エージェントを制御するために使用する言語モデルをロードしましょう。
llm = OpenAI(temperature=0)

# 次に、使用するツールをロードしましょう。`llm-math`ツールはLLMを使用するため

、それを渡す必要があります。
tools = load_tools(["serpapi", "llm-math"], llm=llm)

# 最後に、ツール、言語モデル、使用したいエージェントタイプを指定してエージェントを初期化しましょう。
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# それではテストしてみましょう！
agent.run("What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?")
```

出力:

```
> Entering new AgentExecutor chain...
 I need to find the temperature first, then use the calculator to raise it to the .023 power.
Action: Search
Action Input: "High temperature in SF yesterday"
Observation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 °F (at 1:56 pm) Minimum temperature yesterday: 49 °F (at 1:56 am) Average temperature ...
Thought: I now have the temperature, so I can use the calculator to raise it to the .023 power.
Action: Calculator
Action Input: 57^.023
Observation: Answer: 1.0974509573251117

Thought: I now know the final answer
Final Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.

> Finished chain.
```

以上が、エージェントの基本的な使用方法の日本語版です。このガイドは、エージェントの基本的な概念と使用方法を紹介しています。エージェントは、言語モデルを使用したアプリケーションの開発を容易にするための強力なツールキットであり、このガイドはその使用を始めるための第一歩です。

## 中間地点まとめ

このチュートリアルでは、LangChain を使用してエンドツーエンドの言語モデルアプリケーションを構築する方法について簡単に説明しました。LangChain は、LLMs、プロンプト、チェーン、エージェントなど、言語モデルアプリケーションを構築するための多くのモジュールを提供しています。これらのモジュールは、より複雑なアプリケーションを作成するために組み合わせることも、単純なアプリケーションに対して個別に使用することもできます。

---

### メモリ：チェーンとエージェントに状態を追加する

これまで、私たちが通過したすべてのチェーンとエージェントはステートレスでした。しかし、多くの場合、チェーンやエージェントが以前のインタラクションに関する情報を覚えているという「メモリ」の概念を持つことを望むかもしれません。最も明確で簡単な例は、チャットボットを設計するときです - それがより良い会話を持つために以前のメッセージを覚えていることを望みます。これは「短期記憶」の一種です。より複雑な側面では、チェーン/エージェントが時間をかけて重要な情報のキーピースを覚えていることを想像することができます - これは「長期記憶」の形態です。後者についてのより具体的なアイデアについては、この素晴らしい論文をご覧ください。

LangChain は、この目的のために特別に作成されたいくつかのチェーンを提供します。このノートブックでは、それらのチェーン（ConversationChain）を 2 つの異なるタイプのメモリと一緒に使用する方法を歩いていきます。

デフォルトでは、ConversationChain にはすべての以前の入力/出力を覚えてそれらを渡されるコンテキストに追加するという簡単なタイプのメモリがあります。このチェーンを使用する方法を見てみましょう（verbose=True を設定してプロンプトを見ることができます）。

```python
from langchain import OpenAI, ConversationChain

llm = OpenAI(temperature=0)
conversation = ConversationChain(llm=llm, verbose=True)

output = conversation.predict(input="Hi there!")
print(output)

# -> ' Hello! How are you today?'

output = conversation.predict(input="I'm doing well! Just having a conversation with an AI.")
print(output)

# -> " That's great! What would you like to talk about?"
```

### 言語モデルアプリケーションの構築：チャットモデル

同様に、LLM の代わりにチャットモデルを使用することができます。チャットモデルは言語モデルのバリエーションです。チャットモデルは内部で言語モデルを使用していますが、

彼らが公開するインターフェースは少し異なります：「テキスト入力、テキスト出力」の API を公開するのではなく、入力と出力が「チャットメッセージ」であるインターフェースを公開します。

チャットモデル API は比較的新しいので、まだ正しい抽象化を見つけています。

### チャットモデルからメッセージの完了を取得する

一つ以上のメッセージをチャットモデルに渡すことでチャットの完了を取得することができます。応答はメッセージになります。LangChain で現在サポートされているメッセージのタイプは AIMessage、HumanMessage、SystemMessage、ChatMessage です - ChatMessage は任意のロールパラメータを取る。ほとんどの場合、あなたはただ HumanMessage、AIMessage、SystemMessage を扱うだけです。

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

chat = ChatOpenAI(temperature=0)

# 単一のメッセージを渡すことで完了を取得できます。
chat([HumanMessage(content="Translate this sentence from English to French. I love programming.")])
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})

# OpenAIのgpt-3.5-turboとgpt-4モデルには複数のメッセージを渡すこともできます。
messages = [
    SystemMessage(content="You are a helpful assistant that translates English to French."),
    HumanMessage(content="I love programming.")
]
chat(messages)
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
```

さらに一歩進んで、generate を使用して複数のメッセージセットの完了を生成することができます。これは、追加のメッセージパラメータを持つ LLMResult を返します：

```python
batch_messages = [
    [
        SystemMessage(content="You are a helpful assistant that translates English to French."),
        HumanMessage(content="I love programming.")
    ],
    [
        SystemMessage(content="You are a helpful assistant that translates English to French."),
        HumanMessage(content="I love artificial intelligence.")
    ],
]
result = chat.generate(batch_messages)
result
# -> LLMResult(generations=[[ChatGeneration(text="J'aime programmer.", generation_info=None, message=AIMessage(content="J'aime programmer.", additional_kwargs={})), [ChatGeneration(text="J'aime l'intelligence artificielle.", generation_info=None, message=AIMessage(content="J'aime l'intelligence artificielle.", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})

# このLLMResultからトークンの使用状況などを回復することができます：
result.llm_output['token_usage']
# -> {'prompt_tokens': 57, 'completion_tokens': 20

, 'total_tokens': 77}

```

### チャットプロンプトテンプレート

LLM と同様に、MessagePromptTemplate を使用してテンプレート化を利用することができます。一つ以上の MessagePromptTemplates から ChatPromptTemplate を構築することができます。ChatPromptTemplate の format_prompt を使用すると、これは PromptValue を返します。これを文字列または Message オブジェクトに変換することができます。これは、フォーマットされた値を llm またはチャットモデルの入力として使用するかどうかによります。

便宜上、テンプレートには from_template というメソッドが公開されています。このテンプレートを使用すると、以下のようになります：

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

chat = ChatOpenAI(temperature=0)

template = "You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# フォーマットされたメッセージからチャットの完了を取得する
chat(chat_prompt.format_prompt(input_language="English", output_language="French", text="I love programming.").to_messages())
# -> AIMessage(content="J'aime programmer.", additional_kwargs={})
```

### チャットモデルとチェーン

上記のセクションで説明した LLMChain は、チャットモデルとも使用できます：

```python
from langchain.chat_models import ChatOpenAI
from langchain import LLMChain
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

chat = ChatOpenAI(temperature=0)

template = "You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template = "{text}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

chain = LLMChain(llm=chat, prompt=chat_prompt)
chain.run(input_language="English", output_language="French", text="I love programming.")
# -> "J'aime programmer."
```

### チャットモデルとエージェント

エージェントもチャットモデルと一緒に使用できます。エージェントタイプとして AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION を使用して一つを初期化することができます。

```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.chat_models import ChatOpenAI
from langchain.llms import OpenAI

# 最初に、エージェントを制御するために使用する言語モデルを

ロードします。
llm = OpenAI(temperature=0)

# 次に、エージェントを初期化します。これには、エージェントタイプとエージェントが使用するツールが必要です。
tools = load_tools(llm)
agent = initialize_agent(AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, tools)

# 最後に、エージェントを使用してタスクを実行します。
agent.run(task_description="Translate this sentence from English to French. I love programming.")
# -> "J'aime programmer."
```

以上が、指定された URL の各セクションの日本語訳です。コードや出力結果はそのまま記載しています。

```

```
